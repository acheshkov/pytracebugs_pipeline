Первый слайд

Dear colleagues, I'm Konstantin Kobylkin. 
This is joint work with our team from Krasovskii Institute of Mathematics and Mechanics
and Ural Federal University, Russia.
It is pleasure for us to announce our results at the APSEC conference.
Today we will present a valuable artifact, a dataset of labeled Python source code taken from open-source, for a purpose
to pretrain and finetune complex network architectures for source code analysis tasks. We all know a remarkable success of Transformers and graph networks
for source code analysis. In this talk we will report on the quality and advantages of this dataset, demonstrate its usage in building complex predictive models.

Второй слайд

Defects in software are ubiquitous. They have a variety of undesirable consequences, ranging from 
low readability, maintainability and complexity of source code to unpredictable program and hardware crashes,
leading to significant money losses. Thus, a bug prediction problem is of interest in which one predicts if a source code piece
contains a bug. It is a longstanding problem, dating from early age of computers.

Третий слайд

According to recent surveys, Python is the third popular programming language in the world. 
It became a language of choice in many areas such as web development, statistics and data science.
Python has its own specifics that make it susceptible to bugs such as dynamic typing, absence of formal checks
by interpreter and common static analysis tools. This leads to increasing manual and software debugging efforts, which might be costly.

Четвертый слайд

Many bugs in software reveal themselves by causing the program stop. In Python this is accompanied with a report, that
contains the information about a stack of calls of functions or methods, that might be the first candidates to seek bugs in.
It means that this report is a valuable piece of information that can be exploited by machine learning models to
make more refined source code analysis and sharper bug predictions.

Пятый слайд

Applying machine learning and deep learning models is now widespread in source code analysis tasks
as they proved useful not only in code completion and code maintaining, but also in bug prediction and program repair.
Application of those models comes at cost. They have lots of parameters to be estimated and require very large datasets for their training.


Шестой слайд

To now, actually, there is a strong bottleneck in availability of large public datasets of verified real bugs aimed to build deep learning models
for bug prediction. It is mostly due to the fact that the known datasets are designed for purposes other than bug prediction. Many of the datasets
are collected for a purpose of automatic test generation in which a built test should fail on a buggy piece of code and pass on its fixed version.
Here, there is no need to collect large sized datasets. 

Седьмой слайд

Purposes, underlying the known datasets, also imply certain constraints on their content. Specifically,
datasets, aimed to test generation and program repair, are composed of specific source code, the so called bugfix source code.
Each bugfix contains two consequtive versions of the same source code,
where the first piece contains bugs whereas the second one is an immediate fix of those bugs.

Here, we see an example on this slide of a buggy code piece and its immediate fix:
in the piece above errors might occur if we pass either a non-sequence object as the function first argument or a
non-integer item as its second argument. Its version below contains additional checks of this as a possible fix.


Восьмой слайд

This table presents known public datasets of source code.
According to it, most of the datasets are for Java. As we can see, all Python datasets are small-sized.
Some researches train their deep learning models on artificially generated Python datasets with bugs and then test those models
on small manually curated datasets. The latter approach is possible to apply for simple to introduce bugs. But for many real bugs it does not fit in.

Девятый слайд

The last row in this table presents our contribution: a large public dataset of labeled Python source code, containing examples of real bugs.
The dataset contains both examples of buggy and correct source code in distinction
to the known datasets, composed of bugfixes.

Десятый слайд

This slide presents basic aims of collecting the dataset. It is intended for both training and evaluating complex predictive models
for a binary classification setting of the bug prediction problem with two classes of code pieces: buggy snippets (functions or methods) 
and error-free snippets. The dataset actually contains labeled snippets accordingly to classes in the problem: buggy and correct snippets.

Одиннадцатый слайд

Principles to provide labeling of snippets are roughly as follows. First, a selection is done of well-respected Github repositories, 
maintained by experienced developers. Then, a source code from those commits is extracted, which are linked to Github issues, 
having specific labels, such as bug, error etc.
Buggy parts of those bugfix pairs form a selection of buggy code. To extract correct code, a refined version of the following principle is used:
source code is more probably to be correct if it is stable for a long period of repository commits.

Двенадцатый слайд

Like I mentioned before, our focus, and aim of our dataset, is on specific type of Python bugs that make their corresponding programs stop.
In the table, most present error exception types are presented. Mostly they are attribute absence and empty object related errors.

Тринадцатый слайд

The dataset maintains common workflow from machine learning and statistics. It contains 3 samples: training and validation samples
are used for training models and adjusting their hyperparameters whereas test sample is applied for final model performance estimation.
Training and validation samples contain automatically labeled source code from Github repositories. Test sample is subjected to more thorough
manual evaluation. To provide unbiased performance estimation, training and test samples are made non-overlapping by repositories.

Четырнадцатый слайд

As labeling of training and validation samples is obtained automatically (not by humans), confidence of such labeling must be estimated.
In this regard, we estimate a percentage of refactoring changes, introduced into snippets from bugfix commits. 
To provide a lower bound on this percentage, we evaluate a rate of bugfix pairs of snippets, in which introduced changes
are confined to comments and docstrings. It is 2.6% (2 point six percent). To provide a rough estimate of percentage of refactoring,
we manually validate a random sample of several hundreds of bugfix pairs. Within this sample we observe approximately 10-15 % of refactoring changes.

Пятнадцатый слайд

Like I said before, test sample is manually validated. This slide demonstrates basic principles to guide the manual validation process.
The first principle is that the bug and its fix must be simple to understand. The second principle is that the bug in the buggy snippet should not be
dependency, compatibility and regression bug. To select correct code for test sample, we extract a subset of stable code, containing snippets, being called from
many other snippets in the same repository.

Шестнадцатый слайд

An alternative way to demonstrate quality of the dataset consists in
building predictive models using its data. Here, a BERT-like model is applied, trained on code corpus, containing excerpts
from multiple programming languages, including Python. Specifically, embeddings are computed for source code from the dataset.
Below performances are tabulated on the test sample of the predictive model, trained on these embeddings. The model precision and recall are 
96 and 34 percents in the class of buggy snippets respectively.

Семнадцатый слай

So, we summarize on what we've done. A large Python source code dataset is created. It is intended for training complex predictive 
models for a classical two class setting of the bug prediction problem at the granularity of functions and methods.
The dataset is composed of 24 thousands of snippets of buggy code and 5.7 million of snippets of stable code.
Confidence of labeling of its training and validation samples is 85 percent according to our estimates. 
Confidence of labeling of the test sample is 100 percent.
The dataset is public. It is made online by the link, provided on the slide.


Восемнадцатый слайд

Thank you for your attention



