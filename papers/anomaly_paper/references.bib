@article{AfricEtAl2020,
 author = {Afric, Petar and Sikic, Lucija and Kurdija, Adrian Satja and Silic, Marin},
 title = {REPD: Source code defect prediction as anomaly detection},
 journal = {Journal of Systems and Software},
 volume = {168},
 pages = {110641},
 year = {2020},
 issn = {0164-1212},
 doi = {https://doi.org/10.1016/j.jss.2020.110641},
 url = {http://www.sciencedirect.com/science/article/pii/S0164121220301138},
}

@article{AllamanisEtAl2018,
 author = {Allamanis, Miltiadis and Barr, Earl T. and Devanbu, Premkumar and Sutton, Charles},
 title = {A Survey of Machine Learning for Big Code and Naturalness},
 year = {2018},
 issue_date = {September 2018},
 publisher = {Association for Computing Machinery},
 address = {New York, NY, USA},
 volume = {51},
 number = {4},
 issn = {0360-0300},
 url = {https://doi.org/10.1145/3212695},
 doi = {10.1145/3212695},
 journal = {ACM Comput. Surv.},
 month = jul,
 articleno = {81},
 numpages = {37},
}

@article{AlonEtAl2019vec,
 author = {Alon, Uri and Zilberstein, Meital and Levy, Omer and Yahav, Eran},
 title = {Code2vec: Learning Distributed Representations of Code},
 year = {2019},
 issue_date = {January 2019},
 publisher = {Association for Computing Machinery},
 address = {New York, NY, USA},
 volume = {3},
 number = {POPL},
 url = {https://doi.org/10.1145/3290353},
 doi = {10.1145/3290353},
 journal = {Proc. ACM Program. Lang.},
 month = jan,
 articleno = {40},
 numpages = {29},
}

@inproceedings{AlonEtAl2019seq,
 author = {Alon, Uri and Brody, Shaked and Levy, Omer and Yahav, Eran},
 title = {code2seq: Generating Sequences from Structured Representations of Code}, 
 year = {2019},
 series = {ICLR'19}
}

@article{AzeemEtAl2019,
 author = {Azeem, Muhammad Ilyas and Palomba, Fabio and Shi, Lin and Wang, Qing},
 title = {Machine learning techniques for code smell detection: A systematic literature review and meta-analysis},
 journal = {Information and Software Technology},
 volume = {108},
 pages = {115--138},
 year = {2019},
 issn = {0950-5849},
 doi = {https://doi.org/10.1016/j.infsof.2018.12.009},
 url = {https://www.sciencedirect.com/science/article/pii/S0950584918302623},
 keywords = {Code smells, Machine learning, Systematic literature review},
 abstract = {Background: Code smells indicate suboptimal design or implementation choices in the source code that often lead it to be more change- and fault-prone. Researchers defined dozens of code smell detectors, which exploit different sources of information to support developers when diagnosing design flaws. Despite their good accuracy, previous work pointed out three important limitations that might preclude the use of code smell detectors in practice: (i) subjectiveness of developers with respect to code smells detected by such tools, (ii) scarce agreement between different detectors, and (iii) difficulties in finding good thresholds to be used for detection. To overcome these limitations, the use of machine learning techniques represents an ever increasing research area. Objective: While the research community carefully studied the methodologies applied by researchers when defining heuristic-based code smell detectors, there is still a noticeable lack of knowledge on how machine learning approaches have been adopted for code smell detection and whether there are points of improvement to allow a better detection of code smells. Our goal is to provide an overview and discuss the usage of machine learning approaches in the field of code smells. Method: This paper presents a Systematic Literature Review (SLR) on Machine Learning Techniques for Code Smell Detection. Our work considers papers published between 2000 and 2017. Starting from an initial set of 2456 papers, we found that 15 of them actually adopted machine learning approaches. We studied them under four different perspectives: (i) code smells considered, (ii) setup of machine learning approaches, (iii) design of the evaluation strategies, and (iv) a meta-analysis on the performance achieved by the models proposed so far. Results: The analyses performed show that God Class, Long Method, Functional Decomposition, and Spaghetti Code have been heavily considered in the literature. Decision Trees and Support Vector Machines are the most commonly used machine learning algorithms for code smell detection. Models based on a large set of independent variables have performed well. JRip and Random Forest are the most effective classifiers in terms of performance. The analyses also reveal the existence of several open issues and challenges that the research community should focus on in the future. Conclusion: Based on our findings, we argue that there is still room for the improvement of machine learning techniques in the context of code smell detection. The open issues emerged in this study can represent the input for researchers interested in developing more powerful techniques.}
}

@inproceedings{BryksinEtAl2020,
 author = {Bryksin, Timofey and Petukhov, Victor and Alexin, Ilya and Prikhodko, Stanislav and Shpilman, Alexey and Kovalenko, Vladimir and Povarov, Nikita},
 title = {Using Large-Scale Anomaly Detection on Code to Improve Kotlin Compiler},
 year = {2020},
 isbn = {9781450375177},
 publisher = {Association for Computing Machinery},
 address = {New York, NY, USA},
 url = {https://doi.org/10.1145/3379597.3387447},
 doi = {10.1145/3379597.3387447},
 booktitle = {Proceedings of the 17th International Conference on Mining Software Repositories},
 pages = {455--465},
 numpages = {11},
 location = {Seoul, Republic of Korea},
 series = {MSR '20}
}

@article{ChandolaEtAl2009,
 author = {Chandola, Varun and Banerjee, Arindam and Kumar, Vipin},
 title = {Anomaly Detection: A Survey},
 year = {2009},
 issue_date = {July 2009},
 publisher = {Association for Computing Machinery},
 address = {New York, NY, USA},
 volume = {41},
 number = {3},
 issn = {0360-0300},
 url = {https://doi.org/10.1145/1541880.1541882},
 doi = {10.1145/1541880.1541882},
 abstract = {Anomaly detection is an important problem that has been researched within diverse research areas and application domains. Many anomaly detection techniques have been specifically developed for certain application domains, while others are more generic. This survey tries to provide a structured and comprehensive overview of the research on anomaly detection. We have grouped existing techniques into different categories based on the underlying approach adopted by each technique. For each category we have identified key assumptions, which are used by the techniques to differentiate between normal and anomalous behavior. When applying a given technique to a particular domain, these assumptions can be used as guidelines to assess the effectiveness of the technique in that domain. For each category, we provide a basic anomaly detection technique, and then show how the different existing techniques in that category are variants of the basic technique. This template provides an easier and more succinct understanding of the techniques belonging to each category. Further, for each category, we identify the advantages and disadvantages of the techniques in that category. We also provide a discussion on the computational complexity of the techniques since it is an important issue in real application domains. We hope that this survey will provide a better understanding of the different directions in which research has been done on this topic, and how techniques developed in one area can be applied in domains for which they were not intended to begin with.},
 journal = {ACM Comput. Surv.},
 month = jul,
 articleno = {15},
 numpages = {58},
 keywords = {Anomaly detection, outlier detection}
}

@inproceedings{DevlinEtAl2019,
 author = {Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina},
 title = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
 booktitle = {Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
 month = jun,
 year = {2019},
 address = {Minneapolis, Minnesota},
 publisher = {Association for Computational Linguistics},
 url = {https://www.aclweb.org/anthology/N19-1423},
 doi = {10.18653/v1/N19-1423},
 pages = {4171--4186},
 abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
}

@inproceedings{FengEtAl2020,
 title = {CodeBERT: A Pre-Trained Model for Programming and Natural Languages},
 author = {Feng, Zhangyin  and
      Guo, Daya  and
      Tang, Duyu  and
      Duan, Nan  and
      Feng, Xiaocheng  and
      Gong, Ming  and
      Shou, Linjun  and
      Qin, Bing  and
      Liu, Ting  and
      Jiang, Daxin  and
      Zhou, Ming},
 booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2020},
 month = nov,
 year = {2020},
 address = {Online},
 publisher = {Association for Computational Linguistics},
 url = {https://www.aclweb.org/anthology/2020.findings-emnlp.139},
 doi = {10.18653/v1/2020.findings-emnlp.139},
 pages = {1536--1547},
 abstract = {We present CodeBERT, a bimodal pre-trained model for programming language (PL) and natural language (NL). CodeBERT learns general-purpose representations that support downstream NL-PL applications such as natural language code search, code documentation generation, etc. We develop CodeBERT with Transformer-based neural architecture, and train it with a hybrid objective function that incorporates the pre-training task of replaced token detection, which is to detect plausible alternatives sampled from generators. This enables us to utilize both {``}bimodal{''} data of NL-PL pairs and {``}unimodal data, where the former provides input tokens for model training while the latter helps to learn better generators. We evaluate CodeBERT on two NL-PL applications by fine-tuning model parameters. Results show that CodeBERT achieves state-of-the-art performance on both natural language code search and code documentation generation. Furthermore, to investigate what type of knowledge is learned in CodeBERT, we construct a dataset for NL-PL probing, and evaluate in a zero-shot setting where parameters of pre-trained models are fixed. Results show that CodeBERT performs better than previous pre-trained models on NLPL probing.},
}

@inproceedings{KanadeEtAl2020,
 author    = {Kanade, Aditya and
              Maniatis, Petros and
              Balakrishnan, Gogul and
              Shi, Kensen},
 title     = {Learning and Evaluating Contextual Embedding of Source Code},
 booktitle = {Proceedings of the 37th International Conference on Machine Learning,
              {ICML} 2020, 13--18 July 2020, Virtual Event},
 series    = {Proceedings of Machine Learning Research},
 volume    = {119},
 pages     = {5110--5121},
 publisher = {{PMLR}},
 year      = {2020},
 url       = {http://proceedings.mlr.press/v119/kanade20a.html},
}

@inproceedings{KingmaBa2015,
 author    = {Kingma, Diederik P. Kingma and Ba, Jimmy},
 editor    = {Yoshua Bengio and Yann LeCun},
 title     = {Adam: {A} Method for Stochastic Optimization},
 booktitle = {3rd International Conference on Learning Representations, {ICLR} 2015,
              San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
 year      = {2015},
 url       = {http://arxiv.org/abs/1412.6980},
}

@inproceedings{KingmaWelling2014,
 author = {Kingma, Diederik P. and Welling, Max},
 title = {Auto-Encoding Variational Bayes},
 booktitle = {2nd International Conference on Learning Representations, {ICLR} 2014, Banff, AB, Canada, April 14--16, 2014, Conference Track Proceedings},
 eprint = {http://arxiv.org/abs/1312.6114v10},
 eprintclass = {stat.ML},
 eprinttype = {arXiv},
 file = {:http\://arxiv.org/pdf/1312.6114v10:PDF;:KingmaWelling_Auto-EncodingVariationalBayes.pdf:PDF},
 keywords = {cs.LG stat.ML vae},
 year = 2014,
 abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
}

@article{LiuEtAl2019,
 author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and
           Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
 title = {RoBERTa: A Robustly Optimized BERT Pretraining Approach},
 journal = {arXiv e-prints},
 keywords = {Computer Science - Computation and Language},
 year = 2019,
 month = jul,
 eid = {arXiv:1907.11692},
 pages = {arXiv:1907.11692},
 archivePrefix = {arXiv},
 eprint = {1907.11692},
 primaryClass = {cs.CL},
 adsurl = {https://ui.adsabs.harvard.edu/abs/2019arXiv190711692L},
 adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@inproceedings{MoshtariEtAl2020,
 author = {Moshtari, Sara and Santos, Joanna C.S. and Mirakhorli, Mehdi and Okutan, Ahmet},
 title = {Looking for Software Defects? First Find the Nonconformists},
 booktitle = {2020 IEEE 20th International Working Conference on Source Code Analysis and Manipulation (SCAM)},
 year = {2020},
 volume = {},
 number = {},
 pages = {75--86},
 doi={10.1109/SCAM51674.2020.00014}
}

@article{NeelaEtAl2017,
  author = {Neela, Kamrun Nahar and Ali, Syed Asif and Ami, Amit Seal and Gias, Alim Ul},
  title = {Modeling Software Defects as Anomalies: A Case Study on Promise Repository},
  journal = {JSW},
  volume = {12},
  number = {10},
  pages = {759--772},
  year = {2017}
}

@article{RaychevEtAl2016,
 author = {Raychev, Veselin and Bielik, Pavol and Vechev, Martin},
 title = {Probabilistic Model for Code with Decision Trees},
 year = {2016},
 issue_date = {October 2016},
 publisher = {Association for Computing Machinery},
 address = {New York, NY, USA},
 volume = {51},
 number = {10},
 issn = {0362-1340},
 url = {https://doi.org/10.1145/3022671.2984041},
 doi = {10.1145/3022671.2984041},
 journal = {SIGPLAN Not.},
 month = oct,
 pages = {731--747},
 numpages = {17},
}

@inproceedings{SakuradaYairi2014,
 author = {Sakurada, Mayu and Yairi, Takehisa},
 title = {Anomaly Detection Using Autoencoders with Nonlinear Dimensionality Reduction},
 booktitle = {MLSDA'14},
 year = {2014}
}

@article{ShiEtAl2020,
 author={Shi, Ke and Lu, Yang and Chang, Jingfei and Wei, Zhen},
 title={PathPair2Vec: An AST path pair-based code representation method for defect prediction},
 volume={59},
 url={http://www.sciencedirect.com/science/article/pii/S2590118420300393},
 doi={https://doi.org/10.1016/j.cola.2020.100979},
 journal={Journal of Computer Languages},
 year={2020},
 pages={100979},
 issn={2590-1184},
}

@inproceedings{RezendeEtAl2014,
 author = {Rezende, Danilo Jimenez and Mohamed, Shakir and Wierstra, Daan},
 title = {Stochastic Backpropagation and Approximate Inference in Deep Generative Models},
 booktitle = {Proceedings of the 31st International Conference on Machine Learning},
 pages = {1278--1286},
 year = {2014},
 editor = {Eric P. Xing and Tony Jebara},
 volume = {32},
 number = {2},
 series = {Proceedings of Machine Learning Research},
 address = {Bejing, China},
 month = jul,
 publisher = {PMLR},
 pdf = {http://proceedings.mlr.press/v32/rezende14.pdf},
 url = {http://proceedings.mlr.press/v32/rezende14.html},
 abstract = {We marry ideas from deep neural networks and approximate Bayesian inference to derive a generalised class of deep, directed generative models, endowed with a new algorithm for scalable inference and learning. Our algorithm introduces a recognition model to represent an approximate posterior distribution and uses this for optimisation of a variational lower bound. We develop stochastic backpropagation – rules for gradient backpropagation through stochastic variables – and derive an algorithm that allows for joint optimisation of the parameters of both the generative and recognition models. We demonstrate on several real-world data sets that by using stochastic backpropagation and variational inference, we obtain models that are able to generate realistic samples of data, allow for accurate imputations of missing data, and provide a useful tool for high-dimensional data visualisation.}
}

@article{TongEtAl2018,
 author = {Tong, Haonan and Liu, Bin and Wang, Shihai},
 title = {Software defect prediction using stacked denoising autoencoders and two-stage ensemble learning},
 journal = {Information and Software Technology},
 volume = {96},
 pages = {94--111},
 year = {2018},
 issn = {0950-5849},
 doi = {https://doi.org/10.1016/j.infsof.2017.11.008},
 url = {https://www.sciencedirect.com/science/article/pii/S0950584917300113},
 keywords = {Software defect prediction, Stacked denoising autoencoders, Ensemble learning, Software metrics, Deep learning},
}

@inproceedings{VaswaniEtAl2017,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, undefinedukasz and Polosukhin, Illia},
 title = {Attention is All You Need},
 year = {2017},
 isbn = {9781510860964},
 publisher = {Curran Associates Inc.},
 address = {Red Hook, NY, USA},
 abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.},
 booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
 pages = {6000--6010},
 numpages = {11},
 location = {Long Beach, California, USA},
 series = {NIPS'17}
}

@misc{CCWiki,
  title = {Cyclomatic complexity on Wikipedia},
  howpublished = {\url{https://en.wikipedia.org/wiki/Cyclomatic_complexity}},
  note = {Accessed: 2021-08-29}
}
